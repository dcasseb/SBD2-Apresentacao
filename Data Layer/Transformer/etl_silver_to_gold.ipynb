{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Silver → Gold\n",
    "## Crime Data Analytics - Data Warehouse Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo\n",
    "Este notebook realiza a transformacao dos dados da camada **Silver** para a camada **Gold**, criando um modelo dimensional (Star Schema) otimizado para analises de Business Intelligence.\n",
    "\n",
    "### Arquitetura Medallion\n",
    "```\n",
    "Raw (Bronze) → Silver → Gold\n",
    "    |            |        |\n",
    "    |            |        └─► Modelo Dimensional (Star Schema)\n",
    "    |            └─► Dados Limpos e Padronizados\n",
    "    └─► Dados Brutos\n",
    "```\n",
    "\n",
    "### Entrada e Saida\n",
    "- **Entrada**: `Data Layer/silver/data_silver.csv`\n",
    "- **Saida**: PostgreSQL schema `gold` + CSVs de backup\n",
    "\n",
    "### Tabelas Geradas\n",
    "| Tipo | Tabela | Descricao |\n",
    "|------|--------|----------|\n",
    "| Dimensao | dim_date | Calendario com hierarquia temporal |\n",
    "| Dimensao | dim_time | Horas do dia com periodos |\n",
    "| Dimensao | dim_area | Areas policiais do LAPD |\n",
    "| Dimensao | dim_crime_type | Tipos e categorias de crimes |\n",
    "| Dimensao | dim_victim | Perfil demografico das vitimas |\n",
    "| Fato | fato_crimes | Eventos de crimes com FKs |\n",
    "| Agregacao | agg_crimes_area_period | Metricas por area/mes/periodo |\n",
    "| Agregacao | agg_crimes_type_year | Metricas por tipo/ano |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuracao Inicial\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Importa bibliotecas**: pandas, numpy, sqlalchemy, psycopg2\n",
    "2. **Define funcao `find_project_root()`**: Localiza a raiz do projeto\n",
    "3. **Configura caminhos**: SILVER_PATH, GOLD_PATH, DDL_PATH\n",
    "4. **Cria diretorio Gold**: Para backup em CSV\n",
    "\n",
    "### Bibliotecas necessarias:\n",
    "```bash\n",
    "pip install pandas numpy sqlalchemy psycopg2-binary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracao inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, time\n",
    "import os\n",
    "\n",
    "# SQLAlchemy para conexao com banco\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / 'Data Layer').exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "# Configurar caminhos\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "SILVER_PATH = PROJECT_ROOT / 'Data Layer' / 'silver' / 'data_silver.csv'\n",
    "GOLD_PATH = PROJECT_ROOT / 'Data Layer' / 'gold'\n",
    "DDL_PATH = GOLD_PATH / 'ddl.sql'\n",
    "\n",
    "# Criar diretorio gold se nao existir\n",
    "os.makedirs(GOLD_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Projeto: {PROJECT_ROOT}\")\n",
    "print(f\"Silver: {SILVER_PATH}\")\n",
    "print(f\"Gold: {GOLD_PATH}\")\n",
    "print(f\"DDL: {DDL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuracao do Banco de Dados\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Define parametros de conexao**: host, porta, usuario, senha, database\n",
    "2. **Cria engine SQLAlchemy**: Conexao com PostgreSQL\n",
    "3. **Testa conexao**: Verifica se o banco esta acessivel\n",
    "4. **Executa DDL**: Cria schema e tabelas se nao existirem\n",
    "\n",
    "### Configuracao:\n",
    "Altere as variaveis abaixo conforme seu ambiente:\n",
    "```python\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = 'crime_analytics'\n",
    "DB_USER = 'postgres'\n",
    "DB_PASS = 'sua_senha'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracao do banco de dados PostgreSQL\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'database': 'crime_analytics',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'  # Altere conforme seu ambiente\n",
    "}\n",
    "\n",
    "# String de conexao\n",
    "DATABASE_URL = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "# Criar engine SQLAlchemy\n",
    "engine = None\n",
    "DB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    # Testar conexao\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT version()\"))\n",
    "        version = result.fetchone()[0]\n",
    "        print(f\"Conectado ao PostgreSQL\")\n",
    "        print(f\"Versao: {version[:50]}...\")\n",
    "        DB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Aviso: Banco de dados nao disponivel - {e}\")\n",
    "    print(\"Continuando apenas com exportacao CSV...\")\n",
    "    DB_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Execucao do DDL\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Le arquivo DDL**: `gold/ddl.sql`\n",
    "2. **Executa comandos SQL**: CREATE SCHEMA, CREATE TABLE, CREATE INDEX\n",
    "3. **Cria estrutura**: Schema `gold` com todas as tabelas\n",
    "\n",
    "### Tabelas criadas:\n",
    "- dim_area, dim_crime_type, dim_date, dim_time, dim_victim\n",
    "- fato_crimes\n",
    "- agg_crimes_area_period, agg_crimes_type_year, agg_crime_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar DDL para criar schema e tabelas\n",
    "if DB_AVAILABLE and DDL_PATH.exists():\n",
    "    print(\"Executando DDL...\")\n",
    "    \n",
    "    with open(DDL_PATH, 'r', encoding='utf-8') as f:\n",
    "        ddl_content = f.read()\n",
    "    \n",
    "    # Separar comandos (por ;)\n",
    "    commands = [cmd.strip() for cmd in ddl_content.split(';') if cmd.strip() and not cmd.strip().startswith('--')]\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        for cmd in commands:\n",
    "            if cmd and len(cmd) > 10:  # Ignorar comandos vazios\n",
    "                try:\n",
    "                    conn.execute(text(cmd))\n",
    "                except SQLAlchemyError as e:\n",
    "                    # Ignorar erros de tabela ja existe\n",
    "                    if 'already exists' not in str(e):\n",
    "                        print(f\"   Aviso: {str(e)[:80]}\")\n",
    "    \n",
    "    print(\"   Schema 'gold' criado/verificado com sucesso!\")\n",
    "else:\n",
    "    if not DB_AVAILABLE:\n",
    "        print(\"Banco nao disponivel - pulando DDL\")\n",
    "    else:\n",
    "        print(f\"Arquivo DDL nao encontrado: {DDL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Funcoes Auxiliares de Carga\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **`truncate_table()`**: Limpa tabela antes de inserir (TRUNCATE CASCADE)\n",
    "2. **`load_to_db()`**: Carrega DataFrame no banco usando pandas `to_sql()`\n",
    "3. **`save_csv_backup()`**: Salva CSV como backup\n",
    "\n",
    "### Estrategia de carga:\n",
    "- **TRUNCATE + INSERT**: Limpa e recarrega toda a tabela\n",
    "- **CASCADE**: Remove dependencias de FK automaticamente\n",
    "- **Backup CSV**: Sempre salva arquivo local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_table(table_name: str, schema: str = 'gold'):\n",
    "    \"\"\"Limpa tabela antes de inserir novos dados.\"\"\"\n",
    "    if not DB_AVAILABLE:\n",
    "        return\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {schema}.{table_name} CASCADE\"))\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"   Aviso truncate {table_name}: {e}\")\n",
    "\n",
    "def load_to_db(df: pd.DataFrame, table_name: str, schema: str = 'gold', if_exists: str = 'append'):\n",
    "    \"\"\"Carrega DataFrame no banco de dados PostgreSQL.\"\"\"\n",
    "    if not DB_AVAILABLE:\n",
    "        print(f\"   [DB OFF] {table_name}: apenas CSV\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Limpar tabela antes\n",
    "        truncate_table(table_name, schema)\n",
    "        \n",
    "        # Inserir dados\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            schema=schema,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"   [DB OK] {schema}.{table_name}: {len(df):,} registros\")\n",
    "        return True\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"   [DB ERRO] {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_csv_backup(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"Salva DataFrame como CSV de backup.\"\"\"\n",
    "    filepath = GOLD_PATH / filename\n",
    "    df.to_csv(filepath, index=False)\n",
    "    size_kb = filepath.stat().st_size / 1024\n",
    "    print(f\"   [CSV] {filename}: {size_kb:.1f} KB\")\n",
    "\n",
    "print(\"Funcoes de carga definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Carregamento dos Dados Silver\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Carrega CSV**: Le o arquivo `data_silver.csv`\n",
    "2. **Converte datas**: Transforma colunas para datetime64\n",
    "3. **Exibe estatisticas**: Quantidade de registros e colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados Silver\n",
    "print(\"Carregando dados Silver...\")\n",
    "df_silver = pd.read_csv(SILVER_PATH)\n",
    "\n",
    "# Converter colunas de data\n",
    "df_silver['date_occurred'] = pd.to_datetime(df_silver['date_occurred'], errors='coerce')\n",
    "df_silver['date_reported'] = pd.to_datetime(df_silver['date_reported'], errors='coerce')\n",
    "\n",
    "print(f\"Dados Silver carregados: {len(df_silver):,} registros\")\n",
    "print(f\"Colunas: {len(df_silver.columns)}\")\n",
    "df_silver.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Validacao de Schema e Qualidade\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Valida colunas obrigatorias**: 20+ colunas necessarias\n",
    "2. **Verifica limites de nulos**: Thresholds por coluna critica\n",
    "3. **Valida dominios**: hour (0-23), coordenadas LA, severity\n",
    "4. **Detecta duplicatas**: crime_id unico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacoes de schema e qualidade\n",
    "print(\"Validando schema e qualidade...\")\n",
    "\n",
    "required_cols = [\n",
    "    'crime_id', 'date_occurred', 'date_reported', 'hour',\n",
    "    'area_code', 'area_name',\n",
    "    'crime_code', 'crime_description', 'crime_category', 'crime_severity',\n",
    "    'victim_age_group', 'victim_sex_desc', 'victim_descent_desc',\n",
    "    'victim_age', 'latitude', 'longitude',\n",
    "    'is_violent', 'has_weapon', 'case_closed', 'year', 'month'\n",
    "]\n",
    "\n",
    "null_thresholds = {\n",
    "    'crime_id': 0.00, 'date_occurred': 0.01, 'hour': 0.01,\n",
    "    'area_code': 0.01, 'crime_code': 0.01\n",
    "}\n",
    "\n",
    "def validate_silver_schema(df):\n",
    "    errors, warnings = [], []\n",
    "    \n",
    "    if df.empty:\n",
    "        errors.append(\"Dataset vazio.\")\n",
    "    \n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"Colunas ausentes: {missing}\")\n",
    "    \n",
    "    for col, max_null in null_thresholds.items():\n",
    "        if col in df.columns:\n",
    "            pct = df[col].isna().mean()\n",
    "            if pct > max_null:\n",
    "                errors.append(f\"{col} com {pct:.1%} nulos (limite {max_null:.1%}).\")\n",
    "    \n",
    "    if 'hour' in df.columns:\n",
    "        invalid = ~df['hour'].between(0, 23)\n",
    "        if invalid.any():\n",
    "            errors.append(f\"hour fora de 0-23: {invalid.sum():,} registros.\")\n",
    "    \n",
    "    if 'crime_id' in df.columns:\n",
    "        dup = df['crime_id'].duplicated().sum()\n",
    "        if dup > 0:\n",
    "            warnings.append(f\"crime_id duplicado: {dup:,}\")\n",
    "    \n",
    "    return errors, warnings\n",
    "\n",
    "errors, warnings = validate_silver_schema(df_silver)\n",
    "\n",
    "if warnings:\n",
    "    print(\"Avisos:\")\n",
    "    for w in warnings:\n",
    "        print(f\"   - {w}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"Erros:\")\n",
    "    for e in errors:\n",
    "        print(f\"   - {e}\")\n",
    "    raise ValueError(\"Falha nas validacoes. Corrija antes de continuar.\")\n",
    "else:\n",
    "    print(\"Validacoes concluidas com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Criacao e Carga das Dimensoes\n",
    "\n",
    "As dimensoes sao criadas e carregadas no banco de dados PostgreSQL.\n",
    "Cada dimensao tambem e salva como CSV de backup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.1 Dimensao Data (dim_date)\n",
    "\n",
    "### Atributos:\n",
    "- `sk_date`: Surrogate Key (SERIAL)\n",
    "- `full_date`: Data completa (DATE)\n",
    "- `year`, `quarter`, `month`, `week_of_year`: Hierarquia temporal\n",
    "- `day_of_month`, `day_of_week`, `day_name`, `month_name`: Detalhes\n",
    "- `is_weekend`: Flag fim de semana\n",
    "- `is_holiday`: Flag feriado (default FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao: Data (dim_date)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando dim_date...\")\n",
    "\n",
    "dim_date = df_silver[['date_occurred']].drop_duplicates().dropna().copy()\n",
    "dim_date = dim_date.sort_values('date_occurred').reset_index(drop=True)\n",
    "dim_date['sk_date'] = range(1, len(dim_date) + 1)\n",
    "dim_date['full_date'] = dim_date['date_occurred'].dt.date\n",
    "dim_date['year'] = dim_date['date_occurred'].dt.year\n",
    "dim_date['quarter'] = dim_date['date_occurred'].dt.quarter\n",
    "dim_date['month'] = dim_date['date_occurred'].dt.month\n",
    "dim_date['month_name'] = dim_date['date_occurred'].dt.month_name()\n",
    "dim_date['week_of_year'] = dim_date['date_occurred'].dt.isocalendar().week.astype(int)\n",
    "dim_date['day_of_month'] = dim_date['date_occurred'].dt.day\n",
    "dim_date['day_of_week'] = dim_date['date_occurred'].dt.dayofweek\n",
    "dim_date['day_name'] = dim_date['date_occurred'].dt.day_name()\n",
    "dim_date['is_weekend'] = dim_date['day_of_week'].isin([5, 6])\n",
    "dim_date['is_holiday'] = False\n",
    "\n",
    "# Selecionar colunas para o banco (conforme DDL)\n",
    "dim_date_db = dim_date[['sk_date', 'full_date', 'year', 'quarter', 'month', 'month_name',\n",
    "                        'week_of_year', 'day_of_month', 'day_of_week', 'day_name', \n",
    "                        'is_weekend', 'is_holiday']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(dim_date_db, 'dim_date')\n",
    "save_csv_backup(dim_date_db, 'dim_date.csv')\n",
    "print(f\"   Total: {len(dim_date_db):,} datas unicas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.2 Dimensao Tempo (dim_time)\n",
    "\n",
    "### Atributos:\n",
    "- `sk_time`: Surrogate Key (SERIAL)\n",
    "- `full_time`: Hora completa (TIME)\n",
    "- `hour`: Hora (0-23)\n",
    "- `minute`: Minuto (sempre 0 - granularidade por hora)\n",
    "- `period_of_day`: Madrugada, Manha, Tarde, Noite\n",
    "- `is_rush_hour`: Horario de pico (7-9h, 17-19h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao: Tempo (dim_time)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando dim_time...\")\n",
    "\n",
    "dim_time = pd.DataFrame({'hour': range(24)})\n",
    "dim_time['sk_time'] = dim_time['hour'] + 1\n",
    "dim_time['full_time'] = dim_time['hour'].apply(lambda h: time(h, 0, 0))\n",
    "dim_time['minute'] = 0\n",
    "dim_time['period_of_day'] = dim_time['hour'].apply(\n",
    "    lambda h: 'Madrugada' if h < 6 else 'Manha' if h < 12 else 'Tarde' if h < 18 else 'Noite'\n",
    ")\n",
    "dim_time['is_rush_hour'] = dim_time['hour'].isin([7, 8, 9, 17, 18, 19])\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "dim_time_db = dim_time[['sk_time', 'full_time', 'hour', 'minute', 'period_of_day', 'is_rush_hour']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(dim_time_db, 'dim_time')\n",
    "save_csv_backup(dim_time_db, 'dim_time.csv')\n",
    "print(f\"   Total: {len(dim_time_db):,} horas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.3 Dimensao Area (dim_area)\n",
    "\n",
    "### Atributos:\n",
    "- `sk_area`: Surrogate Key (SERIAL)\n",
    "- `area_code`: Codigo da area LAPD\n",
    "- `area_name`: Nome da area\n",
    "- `region`: North, South, Central, West, Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao: Area (dim_area)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando dim_area...\")\n",
    "\n",
    "dim_area = df_silver[['area_code', 'area_name']].drop_duplicates().copy()\n",
    "dim_area = dim_area.sort_values('area_code').reset_index(drop=True)\n",
    "dim_area['sk_area'] = range(1, len(dim_area) + 1)\n",
    "\n",
    "def get_region(area_name):\n",
    "    north = ['DEVONSHIRE', 'FOOTHILL', 'MISSION', 'NORTH HOLLYWOOD', 'VAN NUYS', 'WEST VALLEY']\n",
    "    south = ['77TH STREET', 'HARBOR', 'SOUTHEAST', 'SOUTHWEST']\n",
    "    central = ['CENTRAL', 'HOLLENBECK', 'RAMPART', 'NEWTON']\n",
    "    west = ['HOLLYWOOD', 'OLYMPIC', 'PACIFIC', 'WEST LA', 'WILSHIRE', 'TOPANGA']\n",
    "    \n",
    "    name_upper = area_name.upper() if area_name else ''\n",
    "    if name_upper in north: return 'North'\n",
    "    elif name_upper in south: return 'South'\n",
    "    elif name_upper in central: return 'Central'\n",
    "    elif name_upper in west: return 'West'\n",
    "    else: return 'Other'\n",
    "\n",
    "dim_area['region'] = dim_area['area_name'].apply(get_region)\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "dim_area_db = dim_area[['sk_area', 'area_code', 'area_name', 'region']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(dim_area_db, 'dim_area')\n",
    "save_csv_backup(dim_area_db, 'dim_area.csv')\n",
    "print(f\"   Total: {len(dim_area_db):,} areas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.4 Dimensao Tipo de Crime (dim_crime_type)\n",
    "\n",
    "### Atributos:\n",
    "- `sk_crime_type`: Surrogate Key (SERIAL)\n",
    "- `crime_code`: Codigo do crime\n",
    "- `crime_description`: Descricao\n",
    "- `crime_category`: Violent Crime, Property Crime, Other\n",
    "- `is_violent`: Flag crime violento\n",
    "- `severity_level`: 1 (Minor), 3 (Serious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao: Tipo de Crime (dim_crime_type)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando dim_crime_type...\")\n",
    "\n",
    "dim_crime_type = df_silver[['crime_code', 'crime_description', 'crime_category', 'crime_severity']].drop_duplicates().copy()\n",
    "dim_crime_type = dim_crime_type.sort_values('crime_code').reset_index(drop=True)\n",
    "dim_crime_type['sk_crime_type'] = range(1, len(dim_crime_type) + 1)\n",
    "dim_crime_type['is_violent'] = dim_crime_type['crime_category'] == 'Violent Crime'\n",
    "dim_crime_type['severity_level'] = dim_crime_type['crime_severity'].map({'Serious': 3, 'Minor': 1}).fillna(1).astype(int)\n",
    "\n",
    "# Selecionar colunas para o banco (remover crime_severity, usar severity_level)\n",
    "dim_crime_type_db = dim_crime_type[['sk_crime_type', 'crime_code', 'crime_description', \n",
    "                                     'crime_category', 'is_violent', 'severity_level']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(dim_crime_type_db, 'dim_crime_type')\n",
    "save_csv_backup(dim_crime_type_db, 'dim_crime_type.csv')\n",
    "print(f\"   Total: {len(dim_crime_type_db):,} tipos de crime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.5 Dimensao Vitima (dim_victim)\n",
    "\n",
    "### Atributos:\n",
    "- `sk_victim`: Surrogate Key (SERIAL)\n",
    "- `age_group`: Faixa etaria\n",
    "- `sex`: M, F, X (CHAR 1)\n",
    "- `descent`: Codigo etnia (CHAR 1)\n",
    "- `descent_description`: Descricao da etnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao: Vitima (dim_victim)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando dim_victim...\")\n",
    "\n",
    "dim_victim = df_silver[['victim_age_group', 'victim_sex_desc', 'victim_descent_desc']].drop_duplicates().copy()\n",
    "dim_victim = dim_victim.reset_index(drop=True)\n",
    "dim_victim['sk_victim'] = range(1, len(dim_victim) + 1)\n",
    "\n",
    "# Mapear sex para codigo de 1 caractere\n",
    "sex_map = {'Male': 'M', 'Female': 'F', 'Unknown': 'X', 'Other': 'X'}\n",
    "dim_victim['sex'] = dim_victim['victim_sex_desc'].map(sex_map).fillna('X')\n",
    "\n",
    "# Mapear descent para codigo de 1 caractere\n",
    "descent_map = {\n",
    "    'Hispanic': 'H', 'White': 'W', 'Black': 'B', 'Asian': 'A',\n",
    "    'Other': 'O', 'Unknown': 'X', 'American Indian': 'I',\n",
    "    'Pacific Islander': 'P', 'Filipino': 'F'\n",
    "}\n",
    "dim_victim['descent'] = dim_victim['victim_descent_desc'].map(descent_map).fillna('X')\n",
    "\n",
    "# Renomear colunas\n",
    "dim_victim = dim_victim.rename(columns={\n",
    "    'victim_age_group': 'age_group',\n",
    "    'victim_descent_desc': 'descent_description'\n",
    "})\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "dim_victim_db = dim_victim[['sk_victim', 'age_group', 'sex', 'descent', 'descent_description']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(dim_victim_db, 'dim_victim')\n",
    "save_csv_backup(dim_victim_db, 'dim_victim.csv')\n",
    "print(f\"   Total: {len(dim_victim_db):,} perfis de vitima\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Criacao e Carga da Tabela Fato\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Cria dicionarios de mapeamento** para surrogate keys\n",
    "2. **Mapeia cada registro Silver** para as FKs correspondentes\n",
    "3. **Adiciona metricas**: latitude, longitude, is_violent\n",
    "4. **Carrega no banco** e salva CSV de backup\n",
    "\n",
    "### Estrutura fato_crimes:\n",
    "- `sk_crime` (PK): Surrogate Key\n",
    "- `nk_crime_id`: Natural Key (ID original)\n",
    "- `sk_area`, `sk_crime_type`, `sk_date`, `sk_time`, `sk_victim` (FKs)\n",
    "- `latitude`, `longitude`: Coordenadas\n",
    "- `is_violent`: Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela Fato: fato_crimes\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando fato_crimes...\")\n",
    "\n",
    "# Criar mapeamentos de surrogate keys\n",
    "date_map = dim_date.set_index('date_occurred')['sk_date'].to_dict()\n",
    "time_map = dim_time.set_index('hour')['sk_time'].to_dict()\n",
    "area_map = dim_area.set_index('area_code')['sk_area'].to_dict()\n",
    "crime_type_map = dim_crime_type.set_index('crime_code')['sk_crime_type'].to_dict()\n",
    "\n",
    "# Criar chave composta para victim\n",
    "dim_victim['victim_key'] = (dim_victim['age_group'].fillna('Unknown') + '|' + \n",
    "                            dim_victim['sex'].fillna('X') + '|' + \n",
    "                            dim_victim['descent'].fillna('X'))\n",
    "victim_map = dim_victim.set_index('victim_key')['sk_victim'].to_dict()\n",
    "\n",
    "# Construir fato\n",
    "fato = pd.DataFrame()\n",
    "fato['sk_crime'] = range(1, len(df_silver) + 1)\n",
    "fato['nk_crime_id'] = df_silver['crime_id'].values\n",
    "fato['sk_date'] = df_silver['date_occurred'].map(date_map).values\n",
    "fato['sk_time'] = df_silver['hour'].map(time_map).values\n",
    "fato['sk_area'] = df_silver['area_code'].map(area_map).values\n",
    "fato['sk_crime_type'] = df_silver['crime_code'].map(crime_type_map).values\n",
    "\n",
    "# Mapear vitima\n",
    "victim_sex_map = {'Male': 'M', 'Female': 'F', 'Unknown': 'X', 'Other': 'X'}\n",
    "descent_code_map = {\n",
    "    'Hispanic': 'H', 'White': 'W', 'Black': 'B', 'Asian': 'A',\n",
    "    'Other': 'O', 'Unknown': 'X', 'American Indian': 'I',\n",
    "    'Pacific Islander': 'P', 'Filipino': 'F'\n",
    "}\n",
    "\n",
    "df_silver['victim_key'] = (\n",
    "    df_silver['victim_age_group'].fillna('Unknown') + '|' + \n",
    "    df_silver['victim_sex_desc'].map(victim_sex_map).fillna('X') + '|' + \n",
    "    df_silver['victim_descent_desc'].map(descent_code_map).fillna('X')\n",
    ")\n",
    "fato['sk_victim'] = df_silver['victim_key'].map(victim_map).values\n",
    "\n",
    "# Metricas\n",
    "fato['latitude'] = df_silver['latitude'].values\n",
    "fato['longitude'] = df_silver['longitude'].values\n",
    "fato['is_violent'] = df_silver['is_violent'].values\n",
    "\n",
    "# Converter FKs para int (tratar NaN)\n",
    "for col in ['sk_date', 'sk_time', 'sk_area', 'sk_crime_type', 'sk_victim']:\n",
    "    fato[col] = fato[col].astype('Int64')  # nullable int\n",
    "\n",
    "# Selecionar colunas para o banco (conforme DDL - sem sk_weapon e sk_premise)\n",
    "fato_db = fato[['sk_crime', 'nk_crime_id', 'sk_area', 'sk_crime_type', \n",
    "                'sk_date', 'sk_time', 'sk_victim', 'latitude', 'longitude', 'is_violent']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(fato_db, 'fato_crimes')\n",
    "save_csv_backup(fato_db, 'fato_crimes.csv')\n",
    "print(f\"   Total: {len(fato_db):,} crimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Criacao e Carga das Agregacoes\n",
    "\n",
    "Tabelas pre-agregadas para otimizar dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.1 Agregacao: Crimes por Area e Periodo\n",
    "\n",
    "### Granularidade: area x ano x mes x periodo_do_dia\n",
    "### Metricas:\n",
    "- `total_crimes`: COUNT\n",
    "- `violent_crimes`: SUM(is_violent)\n",
    "- `property_crimes`: SUM(crime_category = 'Property Crime')\n",
    "- `avg_victim_age`: AVG(victim_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregacao: Crimes por Area e Periodo\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando agg_crimes_area_period...\")\n",
    "\n",
    "# Adicionar period_of_day ao df_silver\n",
    "df_silver['period_of_day'] = df_silver['hour'].apply(\n",
    "    lambda h: 'Madrugada' if h < 6 else 'Manha' if h < 12 else 'Tarde' if h < 18 else 'Noite'\n",
    ")\n",
    "\n",
    "# Flag para property crimes\n",
    "df_silver['is_property'] = df_silver['crime_category'] == 'Property Crime'\n",
    "\n",
    "# Agregar\n",
    "agg_area_period = df_silver.groupby(['area_code', 'year', 'month', 'period_of_day']).agg(\n",
    "    total_crimes=('crime_id', 'count'),\n",
    "    violent_crimes=('is_violent', 'sum'),\n",
    "    property_crimes=('is_property', 'sum'),\n",
    "    avg_victim_age=('victim_age', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Mapear area_code para sk_area\n",
    "agg_area_period['sk_area'] = agg_area_period['area_code'].map(area_map)\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "agg_area_period_db = agg_area_period[['sk_area', 'year', 'month', 'period_of_day',\n",
    "                                       'total_crimes', 'violent_crimes', 'property_crimes', \n",
    "                                       'avg_victim_age']].copy()\n",
    "agg_area_period_db['avg_victim_age'] = agg_area_period_db['avg_victim_age'].round(2)\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(agg_area_period_db, 'agg_crimes_area_period')\n",
    "save_csv_backup(agg_area_period_db, 'agg_crimes_area_period.csv')\n",
    "print(f\"   Total: {len(agg_area_period_db):,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.2 Agregacao: Crimes por Tipo e Ano\n",
    "\n",
    "### Granularidade: tipo_crime x ano\n",
    "### Metricas:\n",
    "- `total_crimes`: COUNT\n",
    "- `weekday_crimes`: COUNT onde is_weekend = FALSE\n",
    "- `weekend_crimes`: COUNT onde is_weekend = TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregacao: Crimes por Tipo e Ano\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando agg_crimes_type_year...\")\n",
    "\n",
    "# Adicionar flag de fim de semana\n",
    "df_silver['is_weekend'] = df_silver['date_occurred'].dt.dayofweek.isin([5, 6])\n",
    "df_silver['is_weekday'] = ~df_silver['is_weekend']\n",
    "\n",
    "# Agregar\n",
    "agg_crime_year = df_silver.groupby(['crime_code', 'year']).agg(\n",
    "    total_crimes=('crime_id', 'count'),\n",
    "    weekday_crimes=('is_weekday', 'sum'),\n",
    "    weekend_crimes=('is_weekend', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Mapear crime_code para sk_crime_type\n",
    "agg_crime_year['sk_crime_type'] = agg_crime_year['crime_code'].map(crime_type_map)\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "agg_crime_year_db = agg_crime_year[['sk_crime_type', 'year', 'total_crimes', \n",
    "                                     'weekday_crimes', 'weekend_crimes']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(agg_crime_year_db, 'agg_crimes_type_year')\n",
    "save_csv_backup(agg_crime_year_db, 'agg_crimes_type_year.csv')\n",
    "print(f\"   Total: {len(agg_crime_year_db):,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.3 Agregacao: Hotspots Geograficos\n",
    "\n",
    "### Granularidade: grid_lat x grid_lon x ano\n",
    "### Metricas:\n",
    "- `total_crimes`: COUNT\n",
    "- `violent_crimes`: SUM(is_violent)\n",
    "- `hotspot_level`: Classificacao baseada em percentis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregacao: Hotspots Geograficos\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Criando agg_crime_hotspots...\")\n",
    "\n",
    "# Filtrar coordenadas validas\n",
    "df_geo = df_silver[(df_silver['latitude'].notna()) & \n",
    "                   (df_silver['longitude'].notna()) &\n",
    "                   (df_silver['latitude'] != 0) & \n",
    "                   (df_silver['longitude'] != 0)].copy()\n",
    "\n",
    "# Criar grid (arredondar para 2 casas decimais ~ 1km)\n",
    "df_geo['grid_lat'] = df_geo['latitude'].round(2)\n",
    "df_geo['grid_lon'] = df_geo['longitude'].round(2)\n",
    "\n",
    "# Agregar\n",
    "agg_hotspots = df_geo.groupby(['grid_lat', 'grid_lon', 'year']).agg(\n",
    "    total_crimes=('crime_id', 'count'),\n",
    "    violent_crimes=('is_violent', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Classificar hotspot_level baseado em percentis\n",
    "def classify_hotspot(total, p75, p90):\n",
    "    if total >= p90:\n",
    "        return 'Critical'\n",
    "    elif total >= p75:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Normal'\n",
    "\n",
    "p75 = agg_hotspots['total_crimes'].quantile(0.75)\n",
    "p90 = agg_hotspots['total_crimes'].quantile(0.90)\n",
    "agg_hotspots['hotspot_level'] = agg_hotspots['total_crimes'].apply(lambda x: classify_hotspot(x, p75, p90))\n",
    "\n",
    "# Selecionar colunas para o banco\n",
    "agg_hotspots_db = agg_hotspots[['grid_lat', 'grid_lon', 'year', 'total_crimes', \n",
    "                                 'violent_crimes', 'hotspot_level']].copy()\n",
    "\n",
    "# Carregar no banco e salvar CSV\n",
    "load_to_db(agg_hotspots_db, 'agg_crime_hotspots')\n",
    "save_csv_backup(agg_hotspots_db, 'agg_crime_hotspots.csv')\n",
    "print(f\"   Total: {len(agg_hotspots_db):,} grids\")\n",
    "print(f\"   Hotspots Critical: {(agg_hotspots_db['hotspot_level'] == 'Critical').sum():,}\")\n",
    "print(f\"   Hotspots High: {(agg_hotspots_db['hotspot_level'] == 'High').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Resumo Final\n",
    "\n",
    "### O que esta celula faz:\n",
    "1. **Lista tabelas no banco** (se disponivel)\n",
    "2. **Lista arquivos CSV** gerados como backup\n",
    "3. **Exibe estatisticas** de registros por tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ETL Silver -> Gold CONCLUIDO!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estatisticas do banco\n",
    "if DB_AVAILABLE:\n",
    "    print(\"\\n[BANCO DE DADOS PostgreSQL]\")\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT table_name, \n",
    "                   (xpath('/row/cnt/text()', xml_count))[1]::text::int as row_count\n",
    "            FROM (\n",
    "                SELECT table_name, \n",
    "                       query_to_xml(format('SELECT COUNT(*) as cnt FROM gold.%I', table_name), false, true, '') as xml_count\n",
    "                FROM information_schema.tables \n",
    "                WHERE table_schema = 'gold'\n",
    "            ) t\n",
    "            ORDER BY table_name\n",
    "        \"\"\"))\n",
    "        print(f\"   {'Tabela':<30} {'Registros':>12}\")\n",
    "        print(\"   \" + \"-\"*42)\n",
    "        for row in result:\n",
    "            print(f\"   {row[0]:<30} {row[1]:>12,}\")\n",
    "else:\n",
    "    print(\"\\n[BANCO DE DADOS] Nao disponivel\")\n",
    "\n",
    "# Estatisticas dos CSVs\n",
    "print(\"\\n[ARQUIVOS CSV - Backup]\")\n",
    "print(f\"   {'Arquivo':<35} {'Tamanho':>10}\")\n",
    "print(\"   \" + \"-\"*45)\n",
    "for f in sorted(GOLD_PATH.glob('*.csv')):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   {f.name:<35} {size_kb:>8.1f} KB\")\n",
    "\n",
    "print(f\"\\nDiretorio Gold: {GOLD_PATH}\")\n",
    "print(\"\\nPipeline finalizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Verificacao de Integridade (Opcional)\n",
    "\n",
    "Executa queries de verificacao para validar a carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificacao de integridade\n",
    "if DB_AVAILABLE:\n",
    "    print(\"Verificando integridade referencial...\")\n",
    "    \n",
    "    checks = [\n",
    "        (\"Fatos sem data\", \"SELECT COUNT(*) FROM gold.fato_crimes WHERE sk_date IS NULL\"),\n",
    "        (\"Fatos sem area\", \"SELECT COUNT(*) FROM gold.fato_crimes WHERE sk_area IS NULL\"),\n",
    "        (\"Fatos sem tipo crime\", \"SELECT COUNT(*) FROM gold.fato_crimes WHERE sk_crime_type IS NULL\"),\n",
    "        (\"Datas unicas\", \"SELECT COUNT(DISTINCT full_date) FROM gold.dim_date\"),\n",
    "        (\"Areas unicas\", \"SELECT COUNT(DISTINCT area_code) FROM gold.dim_area\"),\n",
    "        (\"Tipos crime unicos\", \"SELECT COUNT(DISTINCT crime_code) FROM gold.dim_crime_type\"),\n",
    "    ]\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for name, query in checks:\n",
    "            try:\n",
    "                result = conn.execute(text(query))\n",
    "                value = result.fetchone()[0]\n",
    "                status = \"OK\" if \"sem\" not in name.lower() or value == 0 else \"ALERTA\"\n",
    "                print(f\"   [{status}] {name}: {value:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   [ERRO] {name}: {e}\")\n",
    "    \n",
    "    print(\"\\nVerificacao concluida.\")\n",
    "else:\n",
    "    print(\"Banco nao disponivel - verificacao ignorada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
